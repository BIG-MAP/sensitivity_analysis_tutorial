{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tom9RzVKkrV",
    "tags": []
   },
   "source": [
    "# Sensitivity analysis with GP regression model\n",
    "\n",
    "Now that we are familiar the data and have a trained GP regression model, we can proceed to the actual sensitivity analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lk0Uot9GK4OU"
   },
   "source": [
    "## Dependencies\n",
    "\n",
    "As in the previous notebooks, we start by importing all dependencies.\n",
    "\n",
    "If you are in Colab, you need to install the [pyro](https://pyro.ai/) package by uncommenting and running the line `!pip3 install pyro-ppl` below before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 515,
     "status": "ok",
     "timestamp": 1642348244803,
     "user": {
      "displayName": "jonas busk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7-1gAF7PppPBq1jWtOrRLj_kiVnCZpQDWsCTO4g=s64",
      "userId": "13756499934799797810"
     },
     "user_tz": -60
    },
    "id": "-752nQQfK3Xu"
   },
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "# !pip3 install pyro-ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1394,
     "status": "ok",
     "timestamp": 1642348246658,
     "user": {
      "displayName": "jonas busk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7-1gAF7PppPBq1jWtOrRLj_kiVnCZpQDWsCTO4g=s64",
      "userId": "13756499934799797810"
     },
     "user_tz": -60
    },
    "id": "AH752hkAGieC",
    "outputId": "a4165bc3-8268-4c5b-8b8b-968b57ec0d01"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.contrib.gp as gp\n",
    "\n",
    "pyro.set_rng_seed(0)\n",
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"pyro version: {pyro.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOX52GOBL7U8"
   },
   "source": [
    "## Load the dataset and model parameters\n",
    "\n",
    "We can load the dataset directly from the GitHub URL.\n",
    "Alternatively, the dataset can be loaded from a local file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1010,
     "status": "ok",
     "timestamp": 1642348247664,
     "user": {
      "displayName": "jonas busk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7-1gAF7PppPBq1jWtOrRLj_kiVnCZpQDWsCTO4g=s64",
      "userId": "13756499934799797810"
     },
     "user_tz": -60
    },
    "id": "LiOZx8pWMIA5"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataset_path = \"https://raw.githubusercontent.com/BIG-MAP/sensitivity_analysis_tutorial/main/data/p2d_sei_10k.csv\"\n",
    "# dataset_path = \"data/p2d_sei_10k.csv\"  # local\n",
    "df = pd.read_csv(dataset_path, index_col=0)\n",
    "\n",
    "# store the names of the features and the name of the target variable\n",
    "features = df.columns[:15].tolist()  # use input parameters as features\n",
    "target = \"SEI_thickness(m)\"  # primary target\n",
    "# target = \"Capacity loss (%)\"  # secondary target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6Vqgb4eMgYE"
   },
   "source": [
    "We also need to load the trained model parameters that we saved in the previous notebook. \n",
    "\n",
    "If you are running this notebook in Colab, you need to make the parameter file available in the working directory by uploading it to the Files section to the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1642348247665,
     "user": {
      "displayName": "jonas busk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7-1gAF7PppPBq1jWtOrRLj_kiVnCZpQDWsCTO4g=s64",
      "userId": "13756499934799797810"
     },
     "user_tz": -60
    },
    "id": "56DDLaOlMhow",
    "outputId": "bb5e88b1-ba87-4e05-beef-c69ed81dcb65"
   },
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "if target == \"SEI_thickness(m)\":\n",
    "    pyro.get_param_store().load(\"sgpr_params_sei.p\")\n",
    "if target == \"Capacity loss (%)\":\n",
    "    pyro.get_param_store().load(\"sgpr_params_cap.p\")\n",
    "\n",
    "params = pyro.get_param_store()\n",
    "params.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RW2HoGqfO-hq"
   },
   "source": [
    "## Setup model\n",
    "\n",
    "Setup the model with the trained parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1642348247666,
     "user": {
      "displayName": "jonas busk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7-1gAF7PppPBq1jWtOrRLj_kiVnCZpQDWsCTO4g=s64",
      "userId": "13756499934799797810"
     },
     "user_tz": -60
    },
    "id": "q_FnFDO8PAXl"
   },
   "outputs": [],
   "source": [
    "kernel = gp.kernels.RBF(input_dim=params[\"data.x_train\"].shape[1], variance=params[\"kernel.variance\"], lengthscale=params[\"kernel.lengthscale\"])\n",
    "model = gp.models.SparseGPRegression(params[\"data.x_train\"], params[\"data.y_train\"], kernel, Xu=params[\"Xu\"], noise=params[\"noise\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4AUv4iOePQZV"
   },
   "source": [
    "## Global sensitivity analysis\n",
    "\n",
    "Here we compute the average sensitivity of each input parameter $j$ using the validation dataset.\n",
    "The sensitivities are computed by taking the gradient of the predicted output $f(\\mathbf{x}_n)$ with respect to each input $x_{n,j}$ averaged over the data:\n",
    "\n",
    "$$\n",
    "s_j^f = \\sqrt{ \\frac{1}{N} \\sum_{n=1}^N \\Big( \\frac{\\partial f(\\mathbf{x}_n)}{\\partial x_{n,j}} \\Big)^2 }\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1642348247666,
     "user": {
      "displayName": "jonas busk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7-1gAF7PppPBq1jWtOrRLj_kiVnCZpQDWsCTO4g=s64",
      "userId": "13756499934799797810"
     },
     "user_tz": -60
    },
    "id": "AXV9iKu8PTaA"
   },
   "outputs": [],
   "source": [
    "def sa_autograd(model, X, reduce=None):   \n",
    "    \"\"\"Sensitivity analysis of GP regression model with automatic differentiation.\n",
    "    \n",
    "    Args:\n",
    "        model: Gaussian process regression model\n",
    "        X (tensor): Input data (design matrix)\n",
    "        reduce (string): method used to reduce the sensitivity result: sum, mean, none.\n",
    "    \"\"\"\n",
    "    X.requires_grad = True\n",
    "    # compute gradient of the mean prediction\n",
    "    model.zero_grad()\n",
    "    mean, _ = model(X, full_cov=False, noiseless=False)\n",
    "    gmean = torch.autograd.grad(mean.sum(), X)[0]\n",
    "    # compute gradient of the variance prediction\n",
    "    model.zero_grad()\n",
    "    _, var = model(X, full_cov=False, noiseless=False)\n",
    "    gvar = torch.autograd.grad(var.sum(), X)[0]\n",
    "    X.requires_grad = False\n",
    "    if reduce == \"sum\":\n",
    "        return mean, var, torch.sqrt(torch.sum(gmean**2, dim=0)), torch.sqrt(torch.sum(gvar**2, dim=0))\n",
    "    elif reduce == \"mean\":\n",
    "        return mean, var, torch.sqrt(torch.mean(gmean**2, dim=0)), torch.sqrt(torch.mean(gvar**2, dim=0))\n",
    "    else:\n",
    "        return mean, var, torch.sqrt(gmean**2), torch.sqrt(gvar**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1642348247667,
     "user": {
      "displayName": "jonas busk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7-1gAF7PppPBq1jWtOrRLj_kiVnCZpQDWsCTO4g=s64",
      "userId": "13756499934799797810"
     },
     "user_tz": -60
    },
    "id": "jFBUGhMgQQvx"
   },
   "outputs": [],
   "source": [
    "def plot_sensitivity_bar(s_mean, s_var, features=None, normalise=False):\n",
    "    features = list(range(len(s_mean))) if features is None else features\n",
    "    \n",
    "    # normalise\n",
    "    if normalise:\n",
    "        s_mean = s_mean / s_mean.sum()\n",
    "        s_var = s_var / s_var.sum()\n",
    "\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.title(\"average sensitivities of the mean prediction\")\n",
    "    plt.bar(range(len(features)), s_mean)\n",
    "    plt.xticks(range(len(features)), [f\"x{i}: {f}\" for i,f in enumerate(features)], rotation=90)\n",
    "    plt.xlabel(\"Feature\"); plt.ylabel(\"Sensitivity\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.title(\"average sensitivities of the variance prediction\")\n",
    "    plt.bar(range(len(features)), s_var, color=\"C1\")\n",
    "    plt.xticks(range(len(features)), [f\"x{i}: {f}\" for i,f in enumerate(features)], rotation=90)\n",
    "    plt.xlabel(\"Feature\"); plt.ylabel(\"Sensitivity\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 715
    },
    "executionInfo": {
     "elapsed": 879,
     "status": "ok",
     "timestamp": 1642348248507,
     "user": {
      "displayName": "jonas busk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7-1gAF7PppPBq1jWtOrRLj_kiVnCZpQDWsCTO4g=s64",
      "userId": "13756499934799797810"
     },
     "user_tz": -60
    },
    "id": "yD9wrxecQalZ",
    "outputId": "3b57a57d-a28d-474e-c846-2d3f7e5eb798"
   },
   "outputs": [],
   "source": [
    "_, _, s_mean, s_var = sa_autograd(model, params[\"data.x_valid\"], reduce=\"mean\")\n",
    "\n",
    "plot_sensitivity_bar(s_mean, s_var, features, normalise=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AZeEkVTSzEj"
   },
   "source": [
    "The sensitivities are normalised so they sum to 1 as we are mainly interested in the relative sensitivities.\n",
    "\n",
    "Notice how only a few of the input parameters seem to have high average sensitivity and thus be important.\n",
    "\n",
    "If you made note of any particular input parameters while doing the initial data exploration, how does it compare to the sensitivities? \n",
    "Do the inputs you noticed correspond to the most important inputs found by the sensitivity analysis?\n",
    "\n",
    "If you did the optional analysis of the Bayesian linear model, how does the results compare?\n",
    "\n",
    "Here we used the validation dataset to compute the sensitivities. \n",
    "We could also have sampled new inputs in the appropriate range and used that for the sensitivity analysis (since we do not need to know the true outputs in this analysis). \n",
    "However, since we know the validation data is sampled at random, we would expect to get very similar results.\n",
    "\n",
    "If you are familiar with automatic relevance determination (ARD), you can try to compute feature importances based on ARD defined as the inverse of the kernel length scale parameters (available in `params[\"kernel.lengthscale\"]`) and compare the result with the global sensitivity analysis above.\n",
    "Note that [ARD has been shown to overestimate the importance of nonlinear features](http://proceedings.mlr.press/v89/paananen19a/paananen19a.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlbcpERZUBe-"
   },
   "source": [
    "## Local sensitivity analysis\n",
    "\n",
    "Looking at the sensitivities averaged over the data is useful for identifying the most important inputs.\n",
    "But we might get a better understanding of the data by considering the predictions and sensitivities along the entire range of variation of each input (while keeping all other inputs fixed at their nominal values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1642348248508,
     "user": {
      "displayName": "jonas busk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7-1gAF7PppPBq1jWtOrRLj_kiVnCZpQDWsCTO4g=s64",
      "userId": "13756499934799797810"
     },
     "user_tz": -60
    },
    "id": "OQdJigPYRlEa"
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def predict_sa(x):\n",
    "    return sa_autograd(model, x, reduce=None)\n",
    "\n",
    "def predict_and_plot_1d(d, predict_sa, features, target, x_min, x_max, x_nominal, y_lim=None, n_points=100, figsize=(12,3)):\n",
    "    # create inputs\n",
    "    x = x_nominal\n",
    "    X = x.repeat(n_points, 1)\n",
    "    xd = torch.linspace(x_min[d], x_max[d], n_points)\n",
    "    X[:,d] = xd\n",
    "    # predict point\n",
    "    mean0, var0, s_mean0, s_var0 = predict_sa(x.unsqueeze(0))\n",
    "    mean0, var0, s_mean0, s_var0 = mean0.detach(), var0.detach(), s_mean0.detach(), s_var0.detach()\n",
    "    std0 = var0.sqrt()\n",
    "    # predict grid\n",
    "    mean, var, s_mean, s_var = predict_sa(X)\n",
    "    mean, var, s_mean, s_var = mean.detach(), var.detach(), s_mean.detach(), s_var.detach()\n",
    "    std = var.sqrt().detach()\n",
    "    # plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    # plot mean prediction with uncertainty\n",
    "    plt.subplot(121)\n",
    "    plt.title(\"mean prediction with uncertainty (2*std)\")\n",
    "    plt.plot(xd.numpy(), mean.numpy())\n",
    "    plt.fill_between(xd.numpy(), (mean.numpy() - 2.0 * std.numpy()), (mean.numpy() + 2.0 * std.numpy()), color='C0', alpha=0.3)\n",
    "    plt.axvline(x[d].numpy(), color=\"k\", linewidth=1, label=f\"{mean0.item():.4f} ({std0.item():.4f})\")\n",
    "    plt.xlim((x_min[d], x_max[d]))\n",
    "    if y_lim is not None:\n",
    "        plt.ylim(y_lim)\n",
    "    plt.xlabel(f\"x{d}: {features[d]}\")\n",
    "    plt.ylabel(f\"log y: {target}\")\n",
    "    plt.grid()\n",
    "    plt.legend(loc=4)\n",
    "    # plot sensitivity of mean prediction\n",
    "    plt.subplot(122)\n",
    "    plt.title(\"sensitivity of mean prediction\")\n",
    "    plt.plot(xd.numpy(), s_mean[:, d].numpy())\n",
    "    plt.axvline(x[d].numpy(), color=\"k\", linewidth=1, label=f\"{s_mean0[:,d].item():.4f}\")\n",
    "    plt.xlim((x_min[d], x_max[d]))\n",
    "    plt.ylim((0,5))\n",
    "    plt.xlabel(f\"x{d}: {features[d]}\")\n",
    "    plt.ylabel(\"sensitivity\")\n",
    "    plt.grid()\n",
    "    plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 7433,
     "status": "ok",
     "timestamp": 1642348255937,
     "user": {
      "displayName": "jonas busk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7-1gAF7PppPBq1jWtOrRLj_kiVnCZpQDWsCTO4g=s64",
      "userId": "13756499934799797810"
     },
     "user_tz": -60
    },
    "id": "ds60GTH-Vqln",
    "outputId": "1595491d-f887-4877-d8cf-ef4d2abcf456"
   },
   "outputs": [],
   "source": [
    "for d in range(len(features)):\n",
    "    predict_and_plot_1d(\n",
    "        d,\n",
    "        predict_sa,\n",
    "        features,\n",
    "        target,\n",
    "        x_min=params[\"data.x_min\"].detach().numpy(),\n",
    "        x_max=params[\"data.x_max\"].detach().numpy(),\n",
    "        x_nominal=params[\"data.x_train\"][0].detach(),  # the first training point correponds to the nominal values\n",
    "        y_lim=(params[\"data.y_min\"].item(), params[\"data.y_max\"].item()),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_L72a4XKXTXv"
   },
   "source": [
    "Some of the prediction curves are almost entirely flat because changing their value does not change the output.\n",
    "These correspond to the inputs with low average sensitivity that we identified above.\n",
    "\n",
    "Maybe you also notice that some inputs seem to affect the output along their entire range while some other inputs only seem to affect the output at some specific range of values (for example only high or low values). \n",
    "\n",
    "For each of the important inputs, try to characterise the effect they have on the output:\n",
    " * Is it linear or nonlinear?\n",
    " * Is it sensitive along its entire range of values or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euGO-YNCZHTt"
   },
   "source": [
    "Rather than looking at the inputs in one dimension, we can also plot two inputs against each other in two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1642348255937,
     "user": {
      "displayName": "jonas busk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7-1gAF7PppPBq1jWtOrRLj_kiVnCZpQDWsCTO4g=s64",
      "userId": "13756499934799797810"
     },
     "user_tz": -60
    },
    "id": "wT6Oo8nPV9DY"
   },
   "outputs": [],
   "source": [
    "def predict_and_plot_2d(d0, d1, predict_sa, features, target, x_min, x_max, x_nominal, y_lim=None, n_points=100, n_levels=21, figsize=(12,10)):\n",
    "    # create inputs\n",
    "    x = x_nominal\n",
    "    X = x.repeat(n_points**2, 1)\n",
    "    # setup grid\n",
    "    xd0 = torch.linspace(x_min[d0], x_max[d0], n_points)\n",
    "    xd1 = torch.linspace(x_min[d1], x_max[d1], n_points)\n",
    "    grid_xd0, grid_xd1 = torch.meshgrid(xd0, xd1)        \n",
    "    X[:,d0] = grid_xd0.reshape(len(X))\n",
    "    X[:,d1] = grid_xd1.reshape(len(X))\n",
    "    # predict point\n",
    "    mean0, var0, s_mean0, s_var0 = predict_sa(x.unsqueeze(0))\n",
    "    mean0, var0, s_mean0, s_var0 = mean0.detach(), var0.detach(), s_mean0.detach(), s_var0.detach()\n",
    "    std0 = var0.sqrt()\n",
    "    # predict grid\n",
    "    mean, var, s_mean, s_var = predict_sa(X)\n",
    "    mean, var, s_mean, s_var = mean.detach(), var.detach(), s_mean.detach(), s_var.detach()\n",
    "    std = var.sqrt()\n",
    "\n",
    "    s_mean0_d = (s_mean0[:, d0] + s_mean0[:, d1]).item()\n",
    "    s_var0_d = (s_var0[:, d0] + s_var0[:, d1]).item()\n",
    "\n",
    "    s_mean_d = (s_mean[:, d0] + s_mean[:, d1]).reshape(n_points, n_points)\n",
    "    s_var_d = (s_var[:, d0] + s_var[:, d1]).reshape(n_points, n_points)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    # plot mean prediction\n",
    "    ax = plt.subplot(221)\n",
    "    plt.title(\"mean prediction of log y\")\n",
    "    if y_lim is None:\n",
    "        levels = torch.linspace(mean.min().item(), mean.max().item(), n_levels).numpy()\n",
    "    else:\n",
    "        levels = torch.linspace(y_lim[0], y_lim[1], n_levels).numpy()\n",
    "    plt.contourf(grid_xd0.numpy(), grid_xd1.numpy(), mean.reshape(n_points, n_points).numpy(), levels=levels, cmap=\"plasma\")\n",
    "    plt.axvline(x[d0].numpy(), color=\"k\", linewidth=1, label=f\"{mean0.item():.4f} ({std0.item():.4f})\")\n",
    "    plt.axhline(x[d1].numpy(), color=\"k\", linewidth=1)\n",
    "    plt.xlabel(f\"x{d0}: {features[d0]}\"); plt.ylabel(f\"x{d1}: {features[d1]}\")\n",
    "    plt.colorbar(shrink=0.9)\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('%6.2f'))\n",
    "    plt.legend(loc=4)\n",
    "    # plot uncertainty\n",
    "    ax = plt.subplot(222)\n",
    "    plt.title(\"uncertainty (2*std)\")\n",
    "    levels = torch.linspace(0, 1.0, 21).numpy()\n",
    "    plt.contourf(grid_xd0.numpy(), grid_xd1.numpy(), 2*std.reshape(n_points, n_points).numpy(), levels=levels, cmap=\"plasma\")\n",
    "    plt.axvline(x[d0].numpy(), color=\"k\", linewidth=1, label=f\"{std0.item()*2:.4f}\")\n",
    "    plt.axhline(x[d1].numpy(), color=\"k\", linewidth=1)\n",
    "    plt.xlabel(f\"x{d0}: {features[d0]}\"); plt.ylabel(f\"x{d1}: {features[d1]}\")\n",
    "    plt.colorbar(shrink=0.9)\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('%6.2f'))\n",
    "    plt.legend(loc=4)\n",
    "    # plot sensitivity of mean prediction\n",
    "    ax = plt.subplot(223)\n",
    "    plt.title(\"sensitivity of mean prediction\")\n",
    "    levels = torch.linspace(0, 5.0, 21).numpy()\n",
    "    plt.contourf(grid_xd0.numpy(), grid_xd1.numpy(), s_mean_d.numpy(), levels=levels, cmap=\"plasma\")\n",
    "    plt.axvline(x[d0].numpy(), color=\"k\", linewidth=1, label=f\"{s_mean0_d:.4f}\")\n",
    "    plt.axhline(x[d1].numpy(), color=\"k\", linewidth=1)\n",
    "    plt.xlabel(f\"x{d0}: {features[d0]}\"); plt.ylabel(f\"x{d1}: {features[d1]}\")\n",
    "    plt.colorbar(shrink=0.9)\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('%6.2f'))\n",
    "    plt.legend(loc=4)\n",
    "    # plot sensitivity of uncertainty prediction\n",
    "    ax = plt.subplot(224)\n",
    "    plt.title(\"sensitivity of uncertainty prediction\")\n",
    "    levels = torch.linspace(0, 0.25, 21).numpy()\n",
    "    plt.contourf(grid_xd0.numpy(), grid_xd1.numpy(), s_var_d.numpy(), levels=levels, cmap=\"plasma\")\n",
    "    plt.axvline(x[d0].numpy(), color=\"k\", linewidth=1, label=f\"{s_var0_d:.4f}\")\n",
    "    plt.axhline(x[d1].numpy(), color=\"k\", linewidth=1)\n",
    "    plt.xlabel(f\"x{d0}: {features[d0]}\"); plt.ylabel(f\"x{d1}: {features[d1]}\")\n",
    "    plt.colorbar(shrink=0.9)\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('%6.2f'))\n",
    "    plt.legend(loc=4)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 784
    },
    "executionInfo": {
     "elapsed": 2241,
     "status": "ok",
     "timestamp": 1642348258168,
     "user": {
      "displayName": "jonas busk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7-1gAF7PppPBq1jWtOrRLj_kiVnCZpQDWsCTO4g=s64",
      "userId": "13756499934799797810"
     },
     "user_tz": -60
    },
    "id": "qTbdKI4PaVak",
    "outputId": "474b0699-59c8-4ec3-eb2a-674fbd685b76"
   },
   "outputs": [],
   "source": [
    "predict_and_plot_2d(\n",
    "    0, 2,  # <-- change the input dimensions that are plotted here\n",
    "    predict_sa,\n",
    "    features,\n",
    "    target,\n",
    "    x_min=params[\"data.x_min\"].detach().numpy(),\n",
    "    x_max=params[\"data.x_max\"].detach().numpy(),\n",
    "    x_nominal=params[\"data.x_train\"][0].detach(),  # the first training point correponds to the nominal values\n",
    "    y_lim=(params[\"data.y_min\"].item(), params[\"data.y_max\"].item()),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8e0-4h0rbNWN"
   },
   "source": [
    "Here we plotted input 0 against input 2.\n",
    "You can change the inputs that are plotted in the code above.\n",
    "How about for example inputs 8 and 9?\n",
    "\n",
    "These figures can reveal interesting properties of the data.\n",
    "However, even when plotting two inputs against each other along their entire ranges of values, we still need to assume fixed values for all the other inputs.\n",
    "But changing the value of some sensitive input could potentially interact with other sensitive inputs.\n",
    "Unfortunately, it is difficult to visualize such effects for high dimensional problems like this one.\n",
    "In the next notebooks we will try to mitigate this and make exploring the results of the sensitivity analysis more intuitive by creating interactive plots.\n",
    "\n",
    "As always, we should be aware of the assumptions we made in the analysis and keep them in mind when interpreting the results.\n",
    "* The validity of the results depends on how well the model fits the data.\n",
    "* In this example we made the analysis with regards to the log transformed output and care should be taken if we were to back-transform the results to the original scale since this is a nonlinear transformation and the predictive distribution would no longer be Gaussian."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMs6SXpRqgByQQ8tGM66L2M",
   "collapsed_sections": [],
   "name": "sensitivity_analysis_of_sgpr.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
