{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6125089d-1d04-4a51-b371-f140b9570806",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Bayesian linear regression model [optional]\n",
    "\n",
    "In this notebook we fit a [Bayesian linear regression](https://en.wikipedia.org/wiki/Bayesian_linear_regression) model to the data. \n",
    "This serves mainly as a useful baseline and tells us if there is a strong linear relationship between the inputs and output. \n",
    "If the linear model fits the data well, then perhaps there is no reason to apply a more complicated model!\n",
    "\n",
    "The advantage of using a Bayesian approach over a classical linear regression model in this case is that the Bayesian inference provides us with distributions of all parameters instead of just point estimates, which allows us to perform some additional analyses.\n",
    "\n",
    "Note that this step is optional and not strictly necessary to complete the rest of the tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb08b88b-5f03-4e02-b3d8-50bd7b738d50",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "First we import the required dependencies.\n",
    "\n",
    "If you are in Colab, you need to install the [pyro](https://pyro.ai/) package by uncommenting and running the line `!pip3 install pyro-ppl` below before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896d1a72-6e06-45e7-acf2-599b975f6773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "# !pip3 install pyro-ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae65a64-a095-4fd3-b7a0-90230c3ca1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pyro\n",
    "\n",
    "pyro.set_rng_seed(0)\n",
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"pyro version: {pyro.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a38a1a-e0b6-48de-8bf8-cb4be4e2f431",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load dataset\n",
    "\n",
    "We can load the dataset directly from the GitHub URL.\n",
    "Alternatively, the dataset can be loaded from a local file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c72af5-e986-4a91-a140-d8736a4ff765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataset_path = \"https://raw.githubusercontent.com/BIG-MAP/sensitivity_analysis_tutorial/main/data/p2d_sei_10k.csv\"\n",
    "# dataset_path = \"data/p2d_sei_10k.csv\"  # local\n",
    "df = pd.read_csv(dataset_path, index_col=0)\n",
    "\n",
    "# store the names of the features and the name of the target variable\n",
    "features = df.columns[:15].tolist()  # use input parameters as features\n",
    "target = \"SEI_thickness(m)\"  # primary target\n",
    "# target = \"Capacity loss (%)\"  # secondary target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72255706-693b-4042-96c6-d0ec2084e1a2",
   "metadata": {},
   "source": [
    "## Prepare training and validation data\n",
    "\n",
    "In preparation for training the machine learning model we do a few data transformations:\n",
    "\n",
    "* The target variable is log transformed and normalised to zero mean and unit variance.\n",
    "* The input features are normalised to zero mean and unit variance to make the model parameters easier to learn and to put the inputs on the same scale and thus make results for each input directly comparable. \n",
    "\n",
    "Finally, the data is split into a training and a validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e50d7f-8a98-4673-8999-957346610bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def create_data_split_index(n_data, n_train, n_valid=None, shuffle=False):\n",
    "    \"\"\"Create data split index.\"\"\"\n",
    "    n_valid = n_data - n_train if n_valid is None else n_valid        \n",
    "    index = torch.randperm(n_data) if shuffle else torch.arange(n_data)\n",
    "    split = {\n",
    "        \"train\": index[:n_train],\n",
    "        \"valid\": index[n_train:n_train + n_valid],\n",
    "        \"rest\":  index[n_train + n_valid:],\n",
    "    }\n",
    "    return split\n",
    "\n",
    "def create_normaliser(x, y):\n",
    "    \"\"\"Create data normalisation function\"\"\"\n",
    "    x_mean, x_std = x.mean(axis=0), x.std(axis=0)\n",
    "    y_mean, y_std = y.mean(axis=0), y.std(axis=0)\n",
    "    def normaliser(x, y):\n",
    "        return (x - x_mean) / x_std, (y - y_mean) / y_std\n",
    "    normaliser_params = {\"x_mean\": x_mean, \"x_std\": x_std, \"y_mean\": y_mean, \"y_std\": y_std}\n",
    "    return normaliser, normaliser_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f73ddd-2ac5-4dc4-9080-6213a7e49f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "shuffle = False\n",
    "n_data = len(df)\n",
    "n_train = 5000\n",
    "n_valid = 5000\n",
    "\n",
    "assert n_train + n_valid <= n_data\n",
    "\n",
    "# create data tensors\n",
    "x_data_orig = torch.tensor(df[features].values, dtype=torch.float)\n",
    "y_data_orig = torch.tensor(df[target].values, dtype=torch.float)\n",
    "\n",
    "# log transform y\n",
    "y_data_orig = torch.log(y_data_orig)\n",
    "\n",
    "# create data split index\n",
    "split = create_data_split_index(n_data, n_train, n_valid)\n",
    "\n",
    "# create normalisation function from training split\n",
    "normaliser, normaliser_params = create_normaliser(x_data_orig[split[\"train\"]], y_data_orig[split[\"train\"]])\n",
    "\n",
    "# normalize data\n",
    "x_data, y_data = normaliser(x_data_orig, y_data_orig)\n",
    "\n",
    "# create data splits \n",
    "x_train, y_train = x_data[split[\"train\"]], y_data[split[\"train\"]]\n",
    "x_valid, y_valid = x_data[split[\"valid\"]], y_data[split[\"valid\"]]\n",
    "\n",
    "assert len(x_train) == len(y_train) == n_train\n",
    "assert len(x_valid) == len(y_valid) == n_valid\n",
    "\n",
    "n_bins = 50\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.subplot(121)\n",
    "plt.hist(y_train.numpy(), bins=n_bins)\n",
    "plt.xlabel(\"y_train\")\n",
    "plt.subplot(122)\n",
    "plt.hist(y_valid.numpy(), bins=n_bins)\n",
    "plt.xlabel(\"y_valid\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7db671-5efc-4912-b76e-914710006da3",
   "metadata": {},
   "source": [
    "## Fit Bayesian linear regression model\n",
    "\n",
    "Here we first define a [Bayesian linear regression](https://en.wikipedia.org/wiki/Bayesian_linear_regression) model with normal priors on the parameters.\n",
    "Then we train it on the training data we prepared above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa106a96-1958-44b9-ab46-86064ed5be45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_linear_regression_model(x, y=None):\n",
    "    \"\"\"Bayesian linear regression with normal priors.\"\"\"\n",
    "    # priors\n",
    "    n_features = x.shape[1]\n",
    "    w = pyro.sample(\"weight\", pyro.distributions.Normal(0., 1.).expand([n_features]).to_event(1))\n",
    "    b = pyro.sample(\"bias\", pyro.distributions.Normal(0., 1.))\n",
    "    sigma = pyro.sample(\"sigma\", pyro.distributions.HalfNormal(1.))\n",
    "    # likelihood\n",
    "    mu = (x @ w + b)\n",
    "    with pyro.plate(\"data\", len(x)):\n",
    "        pyro.sample(\"obs\", pyro.distributions.Normal(mu, sigma), obs=y)\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2a1bb0-c787-4644-baac-7b18c7fcbc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "def train(\n",
    "    model,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_valid,\n",
    "    y_valid,\n",
    "    n_steps=1000,\n",
    "    eval_freq=100,\n",
    "):\n",
    "    pyro.clear_param_store()\n",
    "    guide = pyro.infer.autoguide.AutoDiagonalNormal(model)\n",
    "    optimiser = pyro.optim.Adam({\"lr\": 0.01})\n",
    "    svi = pyro.infer.SVI(model, guide, optimiser, loss=pyro.infer.Trace_ELBO())\n",
    "    errors = defaultdict(list)\n",
    "    for step in range(n_steps):\n",
    "        elbo = svi.step(x_train, y_train)\n",
    "        if step == 0 or (step + 1) % eval_freq == 0:\n",
    "            train_loss = svi.evaluate_loss(x_train, y_train) / len(x_train)\n",
    "            valid_loss = svi.evaluate_loss(x_valid, y_valid) / len(x_valid)\n",
    "            errors[\"train_step\"].append(step + 1)\n",
    "            errors[\"train_loss\"].append(train_loss)\n",
    "            errors[\"valid_loss\"].append(valid_loss)\n",
    "            print(f\"[{step + 1:5d}] train loss: {train_loss:7.4f}, valid loss: {valid_loss:7.4f}\")\n",
    "    return guide, errors\n",
    "\n",
    "guide, errors = train(bayesian_linear_regression_model, x_train, y_train, x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686785e6-496c-45fc-995c-c6452d03aeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training curve\n",
    "plt.figure()\n",
    "plt.plot(errors[\"train_step\"], errors[\"train_loss\"], label=\"train loss\")\n",
    "plt.plot(errors[\"train_step\"], errors[\"valid_loss\"], label=\"valid loss\")\n",
    "plt.xlabel(\"training step\"); plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8076aec-b9ff-45c4-b41c-b4f2a0d0115e",
   "metadata": {},
   "source": [
    "## Sample posterior distribution\n",
    "\n",
    "Now that we have a trained model, we can draw samples of the trained model parameters and predictions from the posterior distribution so we can analyse them below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daaf847-54fd-40b5-9fee-24ab005f1acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_posterior(model, guide, x, n_samples=1000):\n",
    "    predictive = pyro.infer.Predictive(model, guide=guide, num_samples=n_samples, return_sites=(\"weight\", \"bias\", \"sigma\", \"obs\", \"_RETURN\",))\n",
    "    raw_samples = predictive(x)\n",
    "    samples = {\"raw_samples\": raw_samples}\n",
    "    for i in range(len(features)):\n",
    "        samples[f\"weight{i}\"] = raw_samples[\"weight\"][:,:,i].squeeze()\n",
    "    samples[\"bias\"] = raw_samples[\"bias\"].squeeze()#.numpy()\n",
    "    samples[\"sigma\"] = raw_samples[\"sigma\"].squeeze()#.numpy()\n",
    "    samples[\"mu\"] = raw_samples[\"_RETURN\"].squeeze()#.numpy()\n",
    "    return samples\n",
    "\n",
    "samples_train = sample_posterior(bayesian_linear_regression_model, guide, x_train)\n",
    "samples_valid = sample_posterior(bayesian_linear_regression_model, guide, x_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b9510b-380e-4af6-9125-330c0c42c0ac",
   "metadata": {},
   "source": [
    "## Posterior predictive distribution\n",
    "\n",
    "First, we check the predictions on the training and validation data to see how well the model fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fb404a-c101-46b6-bd96-483d564ea816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2(y_true, y_pred):\n",
    "    \"\"\"Compute coefficient of determination.\"\"\"\n",
    "    ssr = torch.sum((y_true - y_pred)**2)\n",
    "    sst = torch.sum((y_true - torch.mean(y_true))**2)\n",
    "    return 1 - (ssr / sst)\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    \"\"\"Compute mean absolute error.\"\"\"\n",
    "    return torch.mean(torch.abs(y_true - y_pred))\n",
    "\n",
    "def evaluate_predictions(y_true, y_pred, lim=(-3,3), figsize=(8,4)):\n",
    "    _r2 = r2(y_true, y_pred)  # coefficient of determination\n",
    "    _mae = mae(y_true, y_pred)  # mean absolute error\n",
    "    print(f\"r2: {_r2:.4f}, mae: {_mae:.4f}\\n\")\n",
    "    # plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.subplot(121)\n",
    "    plt.plot(lim, lim, color=\"k\", linestyle=\"--\", linewidth=1)\n",
    "    plt.plot(y_true.numpy(), y_pred.numpy(), \".\", alpha=.1)\n",
    "    plt.xlim(lim); plt.ylim(lim)\n",
    "    plt.xlabel(\"y_true\"); plt.ylabel(\"y_pred\")\n",
    "    plt.grid()\n",
    "    plt.subplot(122)\n",
    "    plt.hist(y_true.numpy() - y_pred.numpy(), bins=20)\n",
    "    plt.xlim(lim)\n",
    "    plt.xlabel(\"y_true - y_pred\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a800f107-0575-4454-a676-4e1b3636b59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on training data\n",
    "evaluate_predictions(y_train, samples_train[\"mu\"].mean(axis=0).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d60a366-7fe9-45a0-8194-30b02d9d264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on validation data\n",
    "evaluate_predictions(y_valid, samples_valid[\"mu\"].mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4815e439-0292-4578-8d38-278cfd55b48f",
   "metadata": {},
   "source": [
    "On the log transformed `SEI_thickness(m)` output, the linear model performs rather well.\n",
    "But there is still room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbaaeb2-07bb-47aa-a002-06e96cba44fe",
   "metadata": {},
   "source": [
    "## Plot posterior distribution\n",
    "\n",
    "Below we first plot the distributions of the model parameters.\n",
    "Parameters that are close to zero have a small effect on the output.\n",
    "\n",
    "We can actually compute the effects by multiplying the (mean) parameters with the corresponding input data and plot the resulting distributions. \n",
    "As expected, the close-to-zero parameters on average have very little effect in the output.\n",
    "\n",
    "Finally we can quantify the feature importances by computing the absolute [t-statistic](https://en.wikipedia.org/wiki/T-statistic) of each parameter distribution:\n",
    "\n",
    "$$\n",
    "t_d = \\frac{\\text{mean}(w_d)}{\\text{std}(w_d)}\n",
    "$$\n",
    "\n",
    "where $d$ denotes the input dimension.\n",
    "\n",
    "If you want to know more about these methods, this chapter on linear regression from the Interpretable Machine Learning book explains them well: https://christophm.github.io/interpretable-ml-book/limo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838e5046-2a98-4af5-a307-4fdb765ae729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_posterior(samples, x, features):\n",
    "    # prepare parameter table\n",
    "    posterior_df = pd.DataFrame.from_dict({k:v.numpy() for k,v in samples.items() if k not in [\"raw_samples\", \"mu\"]})\n",
    "    \n",
    "    # parameter box plot\n",
    "    _ = posterior_df.boxplot(figsize=(posterior_df.shape[1],4), rot=90)\n",
    "    plt.title(\"Posterior distribution of model parameters\")\n",
    "    plt.show()\n",
    "    \n",
    "    # effects box plot (w*x)\n",
    "    effects = (samples[\"raw_samples\"][\"weight\"].mean(axis=0) * x)\n",
    "    effects_df = pd.DataFrame(effects.numpy(), columns=[f\"x{i}: {f}\" for i,f in enumerate(features)])\n",
    "    _ = effects_df.boxplot(figsize=(effects_df.shape[1], 4), rot=90)\n",
    "    plt.title(\"Effects\")\n",
    "    plt.show()\n",
    "    \n",
    "    # feature importance computed as absolute t-statistic |t|\n",
    "    feature_importance = (posterior_df.mean() / posterior_df.std())[[c for c in posterior_df.columns if \"weight\" in c]].abs()\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.bar(range(len(feature_importance)), feature_importance.values)\n",
    "    plt.xticks(range(len(features)), [f\"x{i}: {f}\" for i,f in enumerate(features)], rotation=90)\n",
    "    plt.xlabel(\"Feature\"); plt.ylabel(\"feature importance\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3463d21f-1cc3-46e0-af5d-64fe03e20d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on validation data\n",
    "evaluate_posterior(samples_valid, x_valid, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84200ea-3be5-4d6f-8dcc-4f24fae9edd1",
   "metadata": {},
   "source": [
    "From this analysis, it looks like there are a few important inputs with a large effect on the output, which can already be really useful information. \n",
    "These insights can help us simplify the model by ignoring some of the least important inputs and focus our efforts on the more important inputs.\n",
    "\n",
    "However, there are some important assumptions to keep in mind. \n",
    "* The validity of these results of course depends on how well the linear model fits the data.\n",
    "* In this example we made the analysis with regards to the log transformed output and care should be taken if we were to back-transform the results to the original scale since this is a nonlinear transformation and the predictive distribution would no longer be Gaussian.\n",
    "* Since this is a linear model, it does not reveal if there are any nonlinear effects and thus if the important inputs have the same effect along their entire range of variation or not.\n",
    "Perhaps a nonlinear model could provide some additional insights? We will look at that in the next steps of the tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
