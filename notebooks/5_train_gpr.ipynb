{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rk8JW-uTF0XC"
   },
   "source": [
    "# Train Gaussian process regression model\n",
    "\n",
    "In this notebook we will train the Gaussian process (GP) regression model that we will later use for the sensitivity analysis.\n",
    "\n",
    "We will go through the following steps:\n",
    "\n",
    "* Load the dataset.\n",
    "* Prepare the training and validation data.\n",
    "* Train a GP regression model.\n",
    "* Check the model predictions.\n",
    "* Save the trained model parameters to a file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgOGQl_AHEf_"
   },
   "source": [
    "## Dependencies\n",
    "\n",
    "First we import the dependencies.\n",
    "\n",
    "If you are in Colab, you need to install the [pyro](https://pyro.ai/) package by uncommenting and running the line `!pip3 install pyro-ppl` below before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 218,
     "status": "ok",
     "timestamp": 1642348080942,
     "user": {
      "displayName": "jonas busk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7-1gAF7PppPBq1jWtOrRLj_kiVnCZpQDWsCTO4g=s64",
      "userId": "13756499934799797810"
     },
     "user_tz": -60
    },
    "id": "42A0W46fFhOl"
   },
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "# !pip3 install pyro-ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3382,
     "status": "ok",
     "timestamp": 1642348084631,
     "user": {
      "displayName": "jonas busk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7-1gAF7PppPBq1jWtOrRLj_kiVnCZpQDWsCTO4g=s64",
      "userId": "13756499934799797810"
     },
     "user_tz": -60
    },
    "id": "I84YwXleGSY-",
    "outputId": "37246823-38b1-4e13-e529-bf7418925c6e"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.contrib.gp as gp\n",
    "\n",
    "pyro.set_rng_seed(0)\n",
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"pyro version: {pyro.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtDnIX49G0Ww"
   },
   "source": [
    "## Load dataset\n",
    "\n",
    "We can load the dataset directly from the GitHub URL.\n",
    "Alternatively, the dataset can be loaded from a local file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1642348084956,
     "user": {
      "displayName": "jonas busk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7-1gAF7PppPBq1jWtOrRLj_kiVnCZpQDWsCTO4g=s64",
      "userId": "13756499934799797810"
     },
     "user_tz": -60
    },
    "id": "hnz2yDrqyuv0"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataset_path = \"https://raw.githubusercontent.com/BIG-MAP/sensitivity_analysis_tutorial/main/data/p2d_sei_10k.csv\"\n",
    "# dataset_path = \"data/p2d_sei_10k.csv\"  # local\n",
    "df = pd.read_csv(dataset_path, index_col=0)\n",
    "\n",
    "# store the names of the features and the name of the target variable\n",
    "features = df.columns[:15].tolist()  # use input parameters as features\n",
    "target = \"SEI_thickness(m)\"  # primary target\n",
    "# target = \"Capacity loss (%)\"  # secondary target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmnY6vfXG4bO"
   },
   "source": [
    "## Prepare training and validation data\n",
    "\n",
    "In preparation for training the GP regression model we do a few data transformations:\n",
    "\n",
    "* The target variable is log transformed and normalised to zero mean and unit variance.\n",
    "* The input features are normalised to zero mean and unit variance to make the kernel parameters easier to learn and to put the inputs on the same scale and thus make results for each input directly comparable. \n",
    "\n",
    "Finally, the data is split into a training and a validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1642348084957,
     "user": {
      "displayName": "jonas busk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7-1gAF7PppPBq1jWtOrRLj_kiVnCZpQDWsCTO4g=s64",
      "userId": "13756499934799797810"
     },
     "user_tz": -60
    },
    "id": "PE8A-MTp0gLi"
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def create_data_split_index(n_data, n_train, n_valid=None, shuffle=False):\n",
    "    \"\"\"Create data split index.\"\"\"\n",
    "    n_valid = n_data - n_train if n_valid is None else n_valid        \n",
    "    index = torch.randperm(n_data) if shuffle else torch.arange(n_data)\n",
    "    split = {\n",
    "        \"train\": index[:n_train],\n",
    "        \"valid\": index[n_train:n_train + n_valid],\n",
    "        \"rest\":  index[n_train + n_valid:],\n",
    "    }\n",
    "    return split\n",
    "\n",
    "def create_normaliser(x, y):\n",
    "    \"\"\"Create data normalisation function\"\"\"\n",
    "    x_mean, x_std = x.mean(axis=0), x.std(axis=0)\n",
    "    y_mean, y_std = y.mean(axis=0), y.std(axis=0)\n",
    "    def normaliser(x, y):\n",
    "        return (x - x_mean) / x_std, (y - y_mean) / y_std\n",
    "    normaliser_params = {\"x_mean\": x_mean, \"x_std\": x_std, \"y_mean\": y_mean, \"y_std\": y_std}\n",
    "    return normaliser, normaliser_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "executionInfo": {
     "elapsed": 545,
     "status": "ok",
     "timestamp": 1642348085498,
     "user": {
      "displayName": "jonas busk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7-1gAF7PppPBq1jWtOrRLj_kiVnCZpQDWsCTO4g=s64",
      "userId": "13756499934799797810"
     },
     "user_tz": -60
    },
    "id": "Ad--pHWb1FhX",
    "outputId": "2588172a-b058-4d6a-ad31-7f5dfde2a239"
   },
   "outputs": [],
   "source": [
    "# settings\n",
    "shuffle = False\n",
    "n_data = len(df)\n",
    "n_train = 5000\n",
    "n_valid = 5000\n",
    "\n",
    "assert n_train + n_valid <= n_data\n",
    "\n",
    "# create data tensors\n",
    "x_data_orig = torch.tensor(df[features].values, dtype=torch.float)\n",
    "y_data_orig = torch.tensor(df[target].values, dtype=torch.float)\n",
    "\n",
    "# log transform y\n",
    "y_data_orig = torch.log(y_data_orig)\n",
    "\n",
    "# create data split index\n",
    "split = create_data_split_index(n_data, n_train, n_valid)\n",
    "\n",
    "# create normalisation function from training split\n",
    "normaliser, normaliser_params = create_normaliser(x_data_orig[split[\"train\"]], y_data_orig[split[\"train\"]])\n",
    "\n",
    "# normalise data\n",
    "x_data, y_data = normaliser(x_data_orig, y_data_orig)\n",
    "\n",
    "# create data splits \n",
    "x_train, y_train = x_data[split[\"train\"]], y_data[split[\"train\"]]\n",
    "x_valid, y_valid = x_data[split[\"valid\"]], y_data[split[\"valid\"]]\n",
    "\n",
    "assert len(x_train) == len(y_train) == n_train\n",
    "assert len(x_valid) == len(y_valid) == n_valid\n",
    "\n",
    "n_bins = 50\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.subplot(121)\n",
    "plt.hist(y_train.numpy(), bins=n_bins)\n",
    "plt.xlabel(\"y_train\")\n",
    "plt.subplot(122)\n",
    "plt.hist(y_valid.numpy(), bins=n_bins)\n",
    "plt.xlabel(\"y_valid\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Axt5oeUCMJCP"
   },
   "source": [
    "## Train sparse GP regression model\n",
    "\n",
    "Now we train the GP regression model that we will later use in the sensitivity analysis.\n",
    "Specifically, we use the [SparseGPRegression](https://docs.pyro.ai/en/stable/contrib.gp.html#module-pyro.contrib.gp.models.sgpr) model from the [pyro](https://pyro.ai/) package because we have found it can handle rather large datasets while still being quite fast to train, and it is easy to use with automatic differentiation as we will see later.\n",
    "Please refer to the [pyro documentation](https://docs.pyro.ai/en/stable/contrib.gp.html#module-pyro.contrib.gp.models.sgpr) for details about the model.\n",
    "\n",
    "If at some point you want to apply this method on a small dataset, perhaps you do not need a sparse mode and you can use the simpler [GPRegression](https://docs.pyro.ai/en/stable/contrib.gp.html#module-pyro.contrib.gp.models.gpr) model instead.\n",
    "\n",
    "The model training might take a minute to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1642348085499,
     "user": {
      "displayName": "jonas busk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7-1gAF7PppPBq1jWtOrRLj_kiVnCZpQDWsCTO4g=s64",
      "userId": "13756499934799797810"
     },
     "user_tz": -60
    },
    "id": "LPF-hk4OLJRQ"
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def mnll(loc, scale, targets):\n",
    "    \"\"\"Compute mean negative log likelihood.\"\"\"\n",
    "    log2pi = np.log(2 * np.pi)\n",
    "    loglik = -0.5 * (torch.log(scale) + log2pi + (targets - loc)**2 / scale)\n",
    "    return torch.mean(-loglik)\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    \"\"\"Compute root mean squared error.\"\"\"\n",
    "    return torch.sqrt(torch.mean((y_true - y_pred)**2))\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    \"\"\"Compute mean absolute error.\"\"\"\n",
    "    return torch.mean(torch.abs(y_true - y_pred))\n",
    "\n",
    "def r2(y_true, y_pred):\n",
    "    \"\"\"Compute coefficient of determination.\"\"\"\n",
    "    ssr = torch.sum((y_true - y_pred)**2)\n",
    "    sst = torch.sum((y_true - torch.mean(y_true))**2)\n",
    "    return 1 - (ssr / sst)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, x, y):\n",
    "    \"\"\"Evaluate model.\"\"\"\n",
    "    mean, var = model(x, full_cov=False, noiseless=False)\n",
    "    errors = dict()\n",
    "    errors[\"mnll\"] = mnll(mean, var, y).detach().item()\n",
    "    errors[\"rmse\"] = rmse(y, mean).detach().item()\n",
    "    errors[\"mae\"] = mae(y, mean).detach().item()\n",
    "    errors[\"r2\"] = r2(y, mean).detach().item()\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39851,
     "status": "ok",
     "timestamp": 1642348125347,
     "user": {
      "displayName": "jonas busk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7-1gAF7PppPBq1jWtOrRLj_kiVnCZpQDWsCTO4g=s64",
      "userId": "13756499934799797810"
     },
     "user_tz": -60
    },
    "id": "ATZ9iYww298w",
    "outputId": "828b807f-48f5-48e6-c87d-99dfe81b1f36"
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "\n",
    "def train(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_valid,\n",
    "    y_valid,\n",
    "    n_inducing_points=100,\n",
    "    n_steps=1000,\n",
    "    eval_freq=100,\n",
    "    jitter=1.0e-5\n",
    "):\n",
    "    pyro.clear_param_store()\n",
    "    n_features = x_train.shape[1]\n",
    "\n",
    "    # select the first n training points as the inducing inputs\n",
    "    x_inducing = x_train[:n_inducing_points].clone()\n",
    "    \n",
    "    # initialise the kernel and model\n",
    "    kernel = gp.kernels.RBF(input_dim=n_features, variance=torch.tensor(5.), lengthscale=torch.tensor(n_features * [10.]))\n",
    "    model = gp.models.SparseGPRegression(x_train, y_train, kernel, Xu=x_inducing, jitter=jitter)\n",
    "\n",
    "    # setup optimiser and loss function \n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    loss_fn = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "\n",
    "    errors = defaultdict(list)\n",
    "    for step in range(n_steps):\n",
    "        # train\n",
    "        optimiser.zero_grad()\n",
    "        loss = loss_fn(model.model, model.guide)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        # evaluate\n",
    "        if step == 0 or (step + 1) % eval_freq == 0:\n",
    "            with torch.no_grad():\n",
    "                errors[\"train_step\"].append(step + 1)\n",
    "                errors[\"train_loss\"].append(loss.item() / len(x_train))\n",
    "                for k,v in evaluate(model, x_train, y_train).items():\n",
    "                    errors[\"train_\" + k].append(v)\n",
    "                for k,v in evaluate(model, x_valid, y_valid).items():\n",
    "                    errors[\"valid_\" + k].append(v)\n",
    "            print(f\"[{step + 1:5d}] train loss: {errors['train_loss'][-1]:7.4f} train mnll: {errors['train_mnll'][-1]:7.4f} valid mnll: {errors['valid_mnll'][-1]:7.4f}\")        \n",
    "    return model, errors\n",
    "  \n",
    "\n",
    "model, errors = train(x_train, y_train, x_valid, y_valid, n_steps=800, jitter=1.0e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1642348125347,
     "user": {
      "displayName": "jonas busk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7-1gAF7PppPBq1jWtOrRLj_kiVnCZpQDWsCTO4g=s64",
      "userId": "13756499934799797810"
     },
     "user_tz": -60
    },
    "id": "kNfqwOoOMiI6",
    "outputId": "680b06c0-fc40-49ec-fb28-17386bee019a"
   },
   "outputs": [],
   "source": [
    "# plot training curve\n",
    "plt.figure()\n",
    "plt.plot(errors[\"train_step\"], errors[\"train_mnll\"], label=\"train mnll\")\n",
    "plt.plot(errors[\"train_step\"], errors[\"valid_mnll\"], label=\"valid mnll\")\n",
    "plt.xlabel(\"training step\"); plt.ylabel(\"error\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GF12D715T4Y"
   },
   "source": [
    "We should see the training and validation errors go down with the number of training steps.\n",
    "Go ahead and plot some of the other errors stored in the `errors` dictionary if you like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oX8wpStIlfHj"
   },
   "source": [
    "## Check model predictions\n",
    "\n",
    "Before we do any further analyses, we want to verify that the model fits the training data and makes good predictions on the held-out validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1642348125348,
     "user": {
      "displayName": "jonas busk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7-1gAF7PppPBq1jWtOrRLj_kiVnCZpQDWsCTO4g=s64",
      "userId": "13756499934799797810"
     },
     "user_tz": -60
    },
    "id": "X5JHDtS4zbbo"
   },
   "outputs": [],
   "source": [
    "def evaluate_predictions(y_true, y_pred, lim=(-3,3), figsize=(5,5)):\n",
    "    _r2 = r2(y_true, y_pred)  # coefficient of determination\n",
    "    _mae = mae(y_true, y_pred)  # mean absolute error\n",
    "    print(f\"r2: {_r2:.4f}, mae: {_mae:.4f}\\n\")\n",
    "    # plot y_true against y_pred\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(lim, lim, color=\"k\", linestyle=\"--\", linewidth=1)\n",
    "    plt.plot(y_true, y_pred, \".\", alpha=0.1)\n",
    "    plt.xlabel(\"y_true\"); plt.ylabel(\"y_pred\")\n",
    "    plt.xlim(lim); plt.ylim(lim)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "executionInfo": {
     "elapsed": 393,
     "status": "ok",
     "timestamp": 1642348125721,
     "user": {
      "displayName": "jonas busk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7-1gAF7PppPBq1jWtOrRLj_kiVnCZpQDWsCTO4g=s64",
      "userId": "13756499934799797810"
     },
     "user_tz": -60
    },
    "id": "klrdsztQllau",
    "outputId": "077bbb33-f178-40bf-b18c-682abd0551c2"
   },
   "outputs": [],
   "source": [
    "# evaluate on training data\n",
    "y_pred, y_var = model(x_train, full_cov=False, noiseless=False)\n",
    "evaluate_predictions(y_train.detach(), y_pred.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "executionInfo": {
     "elapsed": 366,
     "status": "ok",
     "timestamp": 1642348126084,
     "user": {
      "displayName": "jonas busk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7-1gAF7PppPBq1jWtOrRLj_kiVnCZpQDWsCTO4g=s64",
      "userId": "13756499934799797810"
     },
     "user_tz": -60
    },
    "id": "7dO9HdiZyHS6",
    "outputId": "4fdf3507-e98f-4882-af44-41b0ec9d0c34"
   },
   "outputs": [],
   "source": [
    "# evaluate on validation data\n",
    "y_pred, y_var = model(x_valid, full_cov=False, noiseless=False)\n",
    "evaluate_predictions(y_valid.detach(), y_pred.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2MlHHbuFama"
   },
   "source": [
    "We should see that the model achieves a r2 value close to 1, indicating the model is able to explain most of the variation in the data, and that the predictions generally correlate with the true target values on both the training and validation data splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4l7Dk00yPIi-"
   },
   "source": [
    "## Save trained model\n",
    "\n",
    "Finally, we save the trained model parameters so we can use the model for analysis later.\n",
    "We additionally save some data parameters that will be useful later.\n",
    "\n",
    "IMPORTANT: If you are running this notebook in Colab, you should make sure to download the saved file as we will need it later in the tutorial series. \n",
    "You can find it in the Files section to the left (the small folder icon) after running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1642348126085,
     "user": {
      "displayName": "jonas busk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7-1gAF7PppPBq1jWtOrRLj_kiVnCZpQDWsCTO4g=s64",
      "userId": "13756499934799797810"
     },
     "user_tz": -60
    },
    "id": "ZuoGiqab8H9S",
    "outputId": "ed40bbf6-0de6-4474-ba38-b0457b6a6733"
   },
   "outputs": [],
   "source": [
    "# store data normalisation parameters\n",
    "pyro.param(\"data.x_mean\", normaliser_params[\"x_mean\"])\n",
    "pyro.param(\"data.x_std\", normaliser_params[\"x_std\"])\n",
    "pyro.param(\"data.y_mean\", normaliser_params[\"y_mean\"])\n",
    "pyro.param(\"data.y_std\", normaliser_params[\"y_std\"])\n",
    "\n",
    "# store data range parameters\n",
    "pyro.param(\"data.x_min\", x_data.min(dim=0)[0])\n",
    "pyro.param(\"data.x_max\", x_data.max(dim=0)[0])\n",
    "pyro.param(\"data.y_min\", y_data.min())\n",
    "pyro.param(\"data.y_max\", y_data.max())\n",
    "\n",
    "# store training and validation data\n",
    "pyro.param(\"data.x_train\", x_train)\n",
    "pyro.param(\"data.y_train\", y_train)\n",
    "pyro.param(\"data.x_valid\", x_valid)\n",
    "pyro.param(\"data.y_valid\", y_valid)\n",
    "\n",
    "# save model parameters in a file\n",
    "print(pyro.get_param_store().keys())\n",
    "if target == \"SEI_thickness(m)\":\n",
    "    pyro.get_param_store().save(\"sgpr_params_sei.p\")\n",
    "if target == \"Capacity loss (%)\":\n",
    "    pyro.get_param_store().save(\"sgpr_params_cap.p\")\n",
    "  \n",
    "# !!! remember to download the saved file !!!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNYghml2cyAIF05V4NSOkWt",
   "collapsed_sections": [],
   "name": "train_sgpr.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
