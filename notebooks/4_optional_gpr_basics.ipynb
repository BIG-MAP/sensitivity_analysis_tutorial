{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f98f0c2-a35f-4ae7-b2a8-5ff29a7a4e11",
   "metadata": {},
   "source": [
    "# Gaussian process regression basics [optional]\n",
    "\n",
    "This notebook presents basic Gaussian process (GP) regression using only the `numpy` package to keep the example as simple as possible. \n",
    "Note that you do not need to know Gaussian processes in detail to be able to complete the rest of the tutorial, so this step is optional.\n",
    "\n",
    "While a complete introduction to Gaussian proccess is beyond the scope of this tutorial, we try to describe the very basics. For more information and theoretical background, we recommend you to take a look at for example the excelent [Gaussian Processes for Machine Learning](http://www.gaussianprocess.org/gpml/) book.\n",
    "The aim here is just to build some intuition about how the model works in practice.\n",
    "\n",
    "In supervised learning we observe input-output pairs $(\\mathbf{x}, y)$ and we assume $y = f(\\mathbf{x})$ for some unknown function $f$, possibly corrupted by noise.\n",
    "The goal of learning is to estimate $f$ as closely as possible from the observed data.\n",
    "The optimal approach would be to estimate a distribution over functions given the data $p(f|\\mathbf{X},\\mathbf{y})$ and use it to make predictions given new inputs $p(y_*|\\mathbf{x}_*,\\mathbf{X},\\mathbf{y}) = \\int p(y_*|f,\\mathbf{x}_*)p(f|\\mathbf{X},\\mathbf{y})df$.\n",
    "\n",
    "A Gaussian process is a generalisation of the Gaussian distribution that describes a distribution over functions and is fully specified by its mean and covariance function:\n",
    "\n",
    "$$\n",
    "f \\sim GP(m(\\mathbf{x}),k(\\mathbf{x},\\mathbf{x'}))\n",
    "$$\n",
    "\n",
    "where $m(\\mathbf{x})$ is the mean function and $k(\\mathbf{x},\\mathbf{x'})$ is the covariance function also known as the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a74fef-1cfd-4a68-963e-8de7c7d94816",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3ccc50-beb4-4d2a-abde-098473b6b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f65067-abb1-48b7-9502-6e1363d82d18",
   "metadata": {},
   "source": [
    "## Example data\n",
    "\n",
    "We start by generating some example data. To keep it simple, we have just one input variable `x` and one output variable `y`. We make `y` a nonlinear function of `x` to better illustrate the flexibility of the GP regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fd72fd-a25e-482a-9268-457cf6f7ee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "N = 20  # number of data points\n",
    "f = lambda x: np.sin(4 * x) * np.sin (5 * x)  # true function\n",
    "x = np.random.uniform(0.1, 1.5, N)  # random values of x\n",
    "y = f(x) + np.random.normal(0, .05, N)  # noisy observations\n",
    "x1 = np.linspace(0, 1.6, 200)  # grid of x values for plotting\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(x1, f(x1), color='k', label=\"true function\")\n",
    "plt.plot(x, y, 'o', color='k', label=\"noisy observations\")\n",
    "plt.xlabel(\"x\"); plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b187a4d9-eeba-4576-aa81-33d51cab2d68",
   "metadata": {},
   "source": [
    "## GP prior distribution\n",
    "\n",
    "Here we define the GP prior distribution. That is the distribution of functions before we observe any data.\n",
    "\n",
    "We will use a very simple zero mean function:\n",
    "\n",
    "$$\n",
    "m(\\mathbf{x}) = 0\n",
    "$$\n",
    "\n",
    "Even with a prior with a zero-mean function, the GP is usually flexible enough to fit a wide variety of functions.\n",
    "\n",
    "We also define the widely used squared exponential kernel (also known as RBF):\n",
    "\n",
    "$$\n",
    "k_{SE}(x, x') = \\sigma_f^2 \\exp \\big( -\\frac{(x - x')^2}{2l^2} \\big)\n",
    "$$\n",
    "\n",
    "where $\\sigma_f^2$ is the variance parameter and $l$ is the length scale parameter.\n",
    "\n",
    "Finally, we define a simple noise kernel in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c107e6-ddef-4cc6-a1d7-55864ed9a5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquaredExponentialKernel:\n",
    "    \n",
    "    def __init__(self, variance, length_scale):\n",
    "        self.variance = variance\n",
    "        self.length_scale = length_scale\n",
    "        \n",
    "    def kernel_function(self, x1, x2):\n",
    "        z = (x1 - x2)**2 / (2 * self.length_scale**2)\n",
    "        return self.variance * np.exp(-z)\n",
    "    \n",
    "    def __call__(self, X, Z=None):\n",
    "        \"\"\"Compute covaraince matrix.\"\"\"\n",
    "        if Z is None:\n",
    "            Z = X\n",
    "        N, M = len(X), len(Z)\n",
    "        K = np.zeros((N, M))\n",
    "        # naive\n",
    "        for i in range(N):\n",
    "            for j in range(M):\n",
    "                K[i, j] = self.kernel_function(X[i], Z[j])\n",
    "        return K\n",
    "    \n",
    "\n",
    "class NoiseKernel:\n",
    "    \n",
    "    def __init__(self, variance):\n",
    "        self.variance = variance\n",
    "        \n",
    "    def __call__(self, X):\n",
    "        \"\"\"Compute covaraince matrix.\"\"\"\n",
    "        return self.variance * np.eye(len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0e3fb2-6e1c-4882-91a4-15057b0ce0d0",
   "metadata": {},
   "source": [
    "Now we can draw some sample functions from the GP prior distribution and plot them. You can try to change the `variance` and `length_scale` parameters in the code below to see how they affect the sampled functions. Try for example to change the `length_scale` in factors of ten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb40430-f870-4890-89d9-588d816c3fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_samples_from_gp_prior(n_samples=5, variance=1.0, length_scale=0.1):\n",
    "    # define kernel\n",
    "    kernel = SquaredExponentialKernel(variance=variance, length_scale=length_scale)\n",
    "    # draw samples\n",
    "    samples = []\n",
    "    for _ in range(n_samples):\n",
    "        samples.append(np.random.multivariate_normal(np.zeros(len(x1)), kernel(x1)))\n",
    "    std = np.sqrt(variance)\n",
    "    # plot\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.fill_between((0.0, 1.6), (2*std, 2*std), (-2*std, -2*std), color=\"C0\", alpha=0.3, label=\"uncertainty (2*std)\")\n",
    "    for i,fx1 in enumerate(samples):\n",
    "        plt.plot(x1, fx1, color=\"C0\", linestyle=\"--\", label=\"prior samples\" if i == 0 else \"\")\n",
    "    plt.xlabel(\"x\"); plt.ylabel(\"y\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "draw_samples_from_gp_prior(n_samples=5, variance=1.0, length_scale=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0759340-6bff-4ad8-be45-3d3593d30207",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GP posterior distribution (with noise)\n",
    "\n",
    "We now condition on the observed data to compute the GP posterior and make predictions of the mean $\\mu$ and the (co)variance $\\Sigma$.\n",
    "For the simple GP regression model, this step can be computed in closed form with the following expressions:\n",
    "\n",
    "\\begin{align}\n",
    "p(\\mathbf{f}_*|\\mathbf{X}_*,\\mathbf{X},\\mathbf{y}) &= \\mathcal{N}(\\mathbf{f}_*|\\mathbf{\\mu},\\mathbf{\\Sigma}) \\\\\n",
    "\\mathbf{\\mu} &= \\mu(\\mathbf{X}_*) + \\mathbf{K}^T_* \\mathbf{K}^{-1} (\\mathbf{y} - \\mu(\\mathbf{X})) = \\mathbf{K}^T_* \\mathbf{K}^{-1} \\mathbf{y} \\\\\n",
    "\\Sigma &= \\mathbf{K}_{**} - \\mathbf{K}^T_* \\mathbf{K}^{-1} \\mathbf{K}_*\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{K} = k(\\mathbf{X},\\mathbf{X}) + \\sigma^2 \\mathbf{I}$, $\\mathbf{K}_* = k(\\mathbf{X},\\mathbf{X}_*)$ and $\\mathbf{K}_{**}=k(\\mathbf{X}_*,\\mathbf{X}_*)$ with covariance function (kernel) $k$ and noise level $\\sigma^2$ and assuming a zero mean function $\\mu(\\mathbf{X})=\\mathbf{0}$.\n",
    "\n",
    "(In practice the Cholesky decomposition can be used instead of the computationally expensive matrix inversion.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea7b0a0-c1d4-4a83-82c8-646022537953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(variance, length_scale, noise_level, n_samples=5):\n",
    "    # kernels\n",
    "    kernel = SquaredExponentialKernel(variance=variance, length_scale=length_scale)\n",
    "    noise = NoiseKernel(variance=noise_level)\n",
    "    # inference\n",
    "    K = kernel(x) + noise(x)\n",
    "    K_inv = np.linalg.inv(K)\n",
    "    K1 = kernel(x, x1)\n",
    "    K11 = kernel(x1, x1)\n",
    "    mu = K1.T @ K_inv @ y\n",
    "    Sigma = K11 - K1.T @ K_inv @ K1\n",
    "    # sample posterior\n",
    "    samples = []\n",
    "    for _ in range(n_samples):\n",
    "        fx1 = np.random.multivariate_normal(mu, Sigma)\n",
    "        samples.append(fx1)\n",
    "    sigma = np.sqrt(Sigma.diagonal())\n",
    "    return mu, sigma, samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02503689-52cb-41f9-9af3-d4a3e3e4859c",
   "metadata": {},
   "source": [
    "Let us plot the model predictions for various values of the length scale parameter to see how it behaves. \n",
    "\n",
    "You can try and change the values of the `variance`, `length_scale` and `noise_level` parameters in the code below to see they affect the predictions of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b47929-1ca7-4cdb-b172-3c7d0fb65acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ls in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    mu, sigma, samples = predict(variance=1.0, length_scale=ls, noise_level=0.01)\n",
    "    # plot\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.title(f\"length_scale={ls}\")\n",
    "    plt.plot(x1, f(x1), color=\"k\", label=\"true function\")\n",
    "    plt.plot(x, y, 'o', color=\"k\", label=\"noisy observations\")\n",
    "    plt.plot(x1, mu, color=\"C0\", label=\"prediction\")\n",
    "    plt.fill_between(x1, mu + 2 * sigma, mu - 2 * sigma, color=\"C0\", alpha=0.3, label=\"uncertainty (2*std)\")\n",
    "    for i,fx1 in enumerate(samples):\n",
    "        plt.plot(x1, fx1, color=\"C0\", linestyle=\"--\", alpha=0.5, label=\"posterior samples\" if i==0 else \"\")\n",
    "    plt.legend(loc=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615635d1-e212-4e2b-b0d5-5cafaf729db2",
   "metadata": {},
   "source": [
    "As you can see, some parameter values make the model fit the data better than others. Thus, the problem of learning a good model of the data consists of finding suitable kernel parameters for the GP model.\n",
    "In practice the kernel parameters can be optimised automatically using gradient based methods. \n",
    "Fortunately there are many GP packages available so we do not have to implement this ourselves. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
